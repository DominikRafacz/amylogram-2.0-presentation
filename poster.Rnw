% Gemini theme
% https://github.com/anishathalye/gemini

\documentclass[final]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[size=a0,scale=1.0,orientation=portrait]{beamerposter}
\usetheme{gemini}
\usecolortheme{gemini}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.025\paperwidth}
\setlength{\colwidth}{0.45\paperwidth}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% ====================
% Title
% ====================

\title{AmyloGram 2.0: MBO in the prediction of amyloid proteins}

\author{Dominik Rafacz* \inst{1} \and Stefan Rödiger \inst{2} \and
Małgorzata Kotulska \inst{3} \and Michał Burdukiewicz \inst{1}}

\institute[shortinst]{\inst{1} Warsaw University of Technology \samelineand \inst{2} Brandenburg Technical University Cottbus-Senftenberg \samelineand \inst{3} Wrocław University of Science and Technology \and
*dominikrafacz@gmail.com $\diamondsuit$ DominikRafacz@github.com
}

% ====================
% Body
% ====================

\begin{document}
\SweaveOpts{concordance=TRUE}

\begin{frame}[t]
\begin{columns}[t]
\separatorcolumn

\begin{column}{\colwidth}

  \begin{block}{Introduction}

 Amyloids are self-aggregating proteins associated with neurodegenerative disorders. The \textit{in silico} identification of amyloid proteins is challenging because their amino acid composition can be extremely variable. Recently, we were able to identify motifs occurring in amyloid sequences and create a machine learning tool, AmyloGram~\cite{AmyloGram}, which has outperformed other predictors of amyloids. AmyloGram focuses on identifying amino acid motifs responsible for aggregation, thus providing researches with insights about structural sources of amyloidogenesis.

  \end{block}
  
  \begin{block}{AmyloGram workflow}
  
    AmyloGram 2.0 is an improved version of the AmyloGram. The improvement in the 2.0 version is the usege of Bayesian hyperparameter tuning.
  
    \begin{figure}
      \includegraphics[width=0.95\textwidth]{figures/ngram_scheme_poster.eps}
    \end{figure}
  \end{block}

  \begin{block}{Model-based optimization}
    The model-based optimization method is one of the possible techniques of discovering optimal hyperparameters for a model in machine learning. Here we treat the model as a function from space of hyperparameters into space of possible model performance values. Computing model performance in numerous points of hyperparameters space is too expensive, so we are trying to estimate the model performance function using so-called surrogate model. It seeks for regions of space where optimal results are more probable to occur based on previous computations. \\
    After initializing surrogate model $\hat{f}$, the following steps are repeated until some of stopping criterions are met:
    \begin{enumerate}
      \item A set of points $\mathbf{x^{(i)}}$ in the hyperparameters space is proosed basing on values of $\hat{f}$.
      \item Value of performance of destination model $f$ is calculated in proposed points $\mathbf{x^{(i)}}$ (model is fit with given set of hyperparameters).
      \item Points $\mathbf{x^{(i)}}$ with corresponding values $\mathbf{y^{(i)}}$ of model $f$ are used to fit surrogate model $\hat{f}$
    \end{enumerate}
    After reaching certain stoping criterion, the best point $\mathbf{x^{(i)}}$ is returned. \\
    MBO framework used by us is \texttt{mlrMBO}~%\citep{mlrMBO}.
    \begin{figure}
      \includegraphics[width=0.95\textwidth]{figures/mbo.eps}
    \end{figure}
    
  \end{block}

\end{column}

\separatorcolumn

\begin{column}{\colwidth}

  \begin{block}{Comparison of methods of hyperparameters tuning}
    \begin{figure}
      \begin{center}
<<tunecomp,warning=FALSE,message=FALSE,echo=FALSE,include=TRUE,results=hide,fig=TRUE, width=7>>=
library(ggplot2)
library(mlrMBO)
library(mlr)
library(randomsearch)
library(paramtest)
library(gridExtra)

set.seed(1590)
ex_fun <- makeSingleObjectiveFunction(
  name = "Sample Function",
  fn = function(x) sin(x)*x^2,
  par.set = makeNumericParamSet("x", len = 1L, lower = -3.8L, upper = 2L),
  global.opt.params = list(x = 0)
)
ctrl <- makeMBOControl(propose.points = 1)
ctrl <- setMBOControlTermination(ctrl, iters = 6L)
ctrl <- setMBOControlInfill(ctrl, crit = makeMBOInfillCritEI(),
                           opt = "focussearch", opt.focussearch.points = 500L)
run <- exampleRun(ex_fun, control = ctrl)

mbo_df <- as.data.frame(run$mbo.res$opt.path)
mbo_optim <- data.frame(x = run$mbo.res$x, y = run$mbo.res$y, method = "MBO")

runRandomSearch <- randomsearch(ex_fun, max.evals = 10L)
random_df <- as.data.frame(runRandomSearch)
random_optim <- getOptPathEl(runRandomSearch, index = getOptPathBestIndex(runRandomSearch))
random_optim <- data.frame(x = random_optim$x$x, y = random_optim$y, method = "random search")

ex_fun_it <- function(iter, x) return(sin(x)*x^2);
runGridSearch <- grid_search(ex_fun_it, params = list(x = seq(-3.8,2)))
grid_df <- data.frame(x = runGridSearch$tests$x, y = unlist(runGridSearch$results))
grid_optim <- data.frame(x = runGridSearch$tests$x[which.min(unlist(runGridSearch$results))],
                         y = min(unlist(runGridSearch$results)), method = "grid search")

points <- rbind(cbind(grid_df[, c("x", "y")], method = "grid search"),
                cbind(random_df[, c("x", "y")], method = "random search"),
                cbind(mbo_df[, c("x", "y")], method = "MBO"))
optims <- rbind(grid_optim, random_optim, mbo_optim)
ggplot(data = points, aes(x = x, y = y)) +
  geom_line(data = run$evals) +
  geom_point() +
  geom_point(data = optims, color = "#d72207") +
  geom_text(data = optims, label = round(optims$y, 2), color = "#d72207", 
            fontface = "bold", vjust = -1) +
  facet_wrap(~method, nrow = 3) +
  theme_bw() +
  xlab("parameter value") +
  ylab("function value") +
  theme(strip.background = element_rect(fill = "#d1e8fc"),
        strip.text = element_text(face = "bold"))
@
      \end{center}
    \end{figure}
  Grid search method seeks for the optimal value of hyperparameter only in equally-spaced points that are defined by the user. Random search method relies on randomly chosen points in parameter space and is empirically better than the previous one. The model-based optimization seeks for improvement of prediction performance using results of its previous iterations.

  \end{block}

  \begin{block}{AmyloGram 2.0 performance}
    \begin{figure}
      \begin{center}
<<res,warning=FALSE,message=FALSE,echo=FALSE,include=TRUE,results =hide, fig=TRUE>>=
results <- readr::read_csv("results/all_models_scores.csv")

library(tidyverse)
results %>%
  select(-pos, -nice_name, -X1, -MCC) %>%
  filter(classifier %in% c("full_alphabet", "best_alphabet", 
                           "class14596_10", "raw_aa_10",
                           "PASTA2", "appnn", "FoldAmyloid")) %>%
  mutate(classifier = fct_reorder(fct_recode(as.factor(classifier), 
                              `AmyloGram 2.0 full\n alphabet` = "full_alphabet",
                              `AmyloGram 2.0 best\n alphabet` = "best_alphabet",
                              `AmyloGram full\n alphabet` = "raw_aa_10",
                              `AmyloGram best\n alphabet` = "class14596_10"),
                              1:7)) %>%
  gather(colname, value, -classifier) %>%
  group_by(colname) %>%
  mutate(maxcol = max(value)) %>%
  ungroup() %>%
  ggplot(aes(x = colname, y = classifier, fill = value,
             colour = as.factor(ifelse(value == maxcol, 1, 2)),
             fontface = as.factor(ifelse(value == maxcol, "bold", "plain")))) +
  geom_bin2d(stat = "identity", color = "#000000") +
  geom_text(aes(label = format(value, digits = 4))) +
  scale_color_manual(values = c("#d72207", "#000000"), guide = FALSE) +
  scale_fill_distiller(palette = "YlOrBr") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) +
  xlab("measure") +
  ylab("classifier") +
  ggtitle("Scores of classifiers")
@
      \end{center}
    \end{figure}
  \end{block}

  \begin{block}{Results and discussion} 
Thanks to the usage of MBO, on the pep424 dataset AmyloGram 2.0 reached AUC 0.91. Moreover, the new version of AmyloGram can to detect aggregation-prone regions in proteins and explain which amino acid motifs are contributing to the amyloidogenicity. We cross-validate motifs and protein regions detected by our tool with experimental data. AmyloGram is available primarily as the web server (http://www.smorfland.uni.wroc.pl/shiny/AmyloGram/) but can be also accessed as standalone software and the R package.
  \end{block}
  \vfill
  
  \begin{block}{Acknowledgements}
  \end{block}
  \vfill

  \begin{block}{Bibliography}
    \tiny{
      \bibliography{references}
    }
  \end{block}
  \vfill
\end{column}

\separatorcolumn
\end{columns}
\end{frame}

\end{document}